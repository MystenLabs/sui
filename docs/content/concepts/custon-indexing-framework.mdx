---
title: Custom Indexing Framework
---

Sui's blockchain generates rich, complex data - transactions, events, object changes, and more. While Sui provides standard APIs to access this data, many applications need customized data processing: tracking specific events, aggregating analytics, building dashboards, or creating specialized databases for their use cases.

The `sui-indexer-alt-framework` is a powerful Rust framework for building high-performance, custom blockchain indexers on Sui. It provides production-ready components for data ingestion, processing, and storage while giving you full control over your indexing logic.

A custom indexer solves this by letting you extract, transform, and store exactly the blockchain data you need. Instead of querying Sui APIs repeatedly or building complex filtering logic, you process the raw blockchain data once and store it in your preferred format.

Leveraging the custom indexing framework can help projects that  track DEX trading volumes, monitor NFT collection activity, build analytics dashboards, create specialized search indexes, aggregate cross-chain bridge data, and more.

## Framework architecture

At a high level, the indexing framework is a streaming pipeline that continuously polls for the latest available checkpoints from a pre-defined data source and streams that checkpoint data to your pipeline's processing logic.

What are checkpoints? Sui organizes blockchain data into checkpoints - batches of transactions that represent consistent blockchain state snapshots. Each checkpoint contains complete transaction details, events, object changes, and execution results with guaranteed ordering. See [Life of a Transaction](../../../concepts/sui-architecture/transaction-lifecycle.mdx#checkpoints) for more information on checkpoints.

![Untitled-2025-06-06-1552.svg](What%20is%20a%20Custom%20Indexer%202346d9dcb4e980a0b0c8d4bc14187a89/Untitled-2025-06-06-1552.svg)

## **Detailed Architecture**

![Untitled-2025-06-06-15521.svg](What%20is%20a%20Custom%20Indexer%202346d9dcb4e980a0b0c8d4bc14187a89/Untitled-2025-06-06-15521.svg)

**Data Sources** (Where checkpoint data comes from):

The framework supports multiple data sources to maximize flexibility.

- **Remote Store** connects to public checkpoint stores like [`https://checkpoints.mainnet.sui.io`](https://checkpoints.mainnet.sui.io/) - the simplest way to get started without running your own infrastructure.
- **Local Files** allows you to process checkpoint files dumped by a local Sui Full node, providing the lowest latency, but it is recommended only for testing as these files are not automatically cleaned up.
- **RPC Endpoint** enables direct connection to Sui Full node RPC endpoint, allowing you to use a trusted data source that you control or to connect to networks (like devnet) that lack a remote store

*→ Different data source configurations are covered in the section [Checkpoint Data Sources](https://www.notion.so/Checkpoint-Data-Sources-2336d9dcb4e980ffa5c7eabb07958007?pvs=21)* 

**Ingestion Layer** (Framework-managed):

This layer handles the complex task of reliably fetching and distributing checkpoint data. The `Broadcaster` receives checkpoints from data sources and efficiently distributes them to multiple processing pipelines running in parallel - essential for indexers that run multiple different data processing workflows simultaneously. 

The `Regulator` acts as a smart coordinator, controlling data flow rates by directing the `Broadcaster` on which checkpoints to fetch. It’s back-pressured by the high watermarks reported by subscribers, so that we don’t fetch too many through `Broadcaster`.

**Processing Layer** (Framework-managed + Your Code):

This is where your custom logic integrates with the framework. The **Pipeline Framework Components** are the heart of the system - it orchestrates checkpoint processing, manages concurrency to maximize throughput, maintains watermarks to track progress and ensure data consistency, and coordinates the entire data flow from ingestion to storage. The framework components adapts based on your chosen pipeline type: 

- **Concurrent pipelines** use components like `Collector`, `Committer`, and `Pruner` for high-throughput out-of-order processing
- **Sequential pipelines** use different components optimized for in-order processing with batching capabilities.

The framework then **exposes specific interfaces** that you implement to define your data processing logic. Some common APIs are:

- `process()` - Transform raw checkpoint data (transactions, events, object changes) into your desired database rows. This is where you extract meaningful information, filter relevant data, and format it for storage.
    
    [https://github.com/MystenLabs/sui/blob/main/crates/sui-types/src/full_checkpoint_content.rs#L19-L23](https://github.com/MystenLabs/sui/blob/main/crates/sui-types/src/full_checkpoint_content.rs#L19-L23)
    
- `commit()` - Store your processed data to the database with proper transaction handling. The framework calls this with batches of processed data for efficient bulk operations.
- `prune()` - Clean up old data based on your retention policies (optional). Useful for managing database size by removing outdated records while preserving recent data.

*→* Pipeline types (Concurrent vs Sequential) and their trade-offs are detailed *in the section: [Concurrent vs Sequential pipelines](https://www.notion.so/Concurrent-vs-Sequential-pipelines-2336d9dcb4e980f1a0a7c1e3c33071fa?pvs=21)* 

**Storage Layer**:

The framework abstracts database operations through a flexible storage layer. **PostgreSQL** comes with built-in support using Diesel ORM, providing production-ready database operations, connection pooling, migrations, and watermark management out of the box. For **Custom DB** implementations, you can implement the framework's storage interfaces to use any database - MongoDB for document storage, ClickHouse for analytics, or any other database that fits your needs.

*→* Implementing custom DB (Bring Your Own Store) is covered in the section *[**Bring Your Own Store (BYOS)**](https://www.notion.so/Bring-Your-Own-Store-BYOS-2336d9dcb4e980db9a06c88248125910?pvs=21).*

# **Single Program, Multiple Threads**

Your indexer runs as **one executable program** with multiple coordinated background tasks (threads). The ingestion layer polls for new checkpoints while your processing pipelines transform and store data - all within the same process.