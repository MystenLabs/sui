---
title: Custom Indexing on Walrus Blob Attributes
description: Walrus is a content-addressable storage protocol, where data is retrieved using a unique identifier derived from the content itself, rather than from a file path or location. Integrating a custom Sui Indexer with Walrus can provide novel user experiences.
---

This topic covers integration of the Sui Indexer with the Walrus protocol. Walrus is a decentralized storage and data availability protocol designed specifically for large binary files, or blobs. Walrus is operated and controlled by the Walrus Foundation. For the most accurate and up-to-date inormation on Walrus, consult the official [Walrus Docs](https://docs.wal.app/).

Walrus is a content-addressable storage protocol, where data is retrieved using a unique identifier derived from the content itself, rather than from a file path or location. Each blob file stored on Walrus is represented by a corresponding [`Blob` object on Sui](https://docs.wal.app/dev-guide/sui-struct.html#blob-and-storage-objects), and this associated object can optionally have a `Metadata` [dynamic field](../../../../concepts/dynamic-fields.mdx). If set, this dynamic field is [a mapping of key-value attribute pairs](https://docs.wal.app/usage/client-cli.html?highlight=attribute#blob-attributes).

OpenDAL is an open-source, cross-platform, unified abstraction layer that provides a standardized API for interacting with a variety of storage backends (local or remote) without needing to know their implementation details. Support for OpenDAL `read`, `write`, `delete`, `list`, and `stat` operations is available based on a path prefix on Walrus. 

As an example, assume a service exists that enables a user to upload files by file path. Behind the scenes, the service uploads content to Walrus, which creates a new `Blob` object on Sui. The service then creates a `Metadata` dynamic field on that Blob object and sets the `path` value. Additional uploads to the same file path are allowed, and are considered newer versions of the same file when indexing the chain. The service also has a `delete` API that accepts a file path and optional version, which deletes the latest `Metadata` and `Blob` object on Sui or at a particular version, if specified.

Your task is to enhance this service with additional abilities. The service should also enable users to:

- Provide a file path to retrieve the file at that location. By default, this references the latest version of the file.
- Provide a file path and version to view the file.
- Provide a path prefix to filter their files.
- See a paginated list of all their files.

To support these features, you can use the custom indexer framework to build an indexer that listens to these object changes, maintaining a mapping of paths to `Blob` IDs. Other dApps can then choose to extend and index metadata for their own needs. 

## Data modeling

You can fulfill the remaining requirements with two tables. The `walrus_blob` table has checkpoint granularity, so if multiple modifications occur within a single checkpoint, the latest entry persists. On the other hand, the append-only `walrus_blob_historical` table records all modifications at version, and uses the lamport version of the transaction it was deleted with or wrapped in.

When you delete metadata on chain, the `walrus_blob` table records this transaction by updating the existing entry’s `cp_sequence_number` value and setting `deleted = TRUE`. Conversely, the `walrus_blob_historical` table writes a new record with most of the fields set to `NULL`. These records serve as tombstone rows that the pruner deletes.

Table and index creation looks like the following:

```sql
-- This table maps file paths to their latest dynamic field metadata. The dynamic_field_id
-- references a Metadata dynamic field on a Sui Blob object, which provides access to both the
-- blob_id pointing to the actual file contents and the address_owner of the Blob object.
CREATE TABLE IF NOT EXISTS walrus_blob (
    -- Address that owns the Blob object on Sui
    address_owner               BYTEA         NOT NULL,
    -- File path associated with the Walrus Blob
    file_path                   TEXT          NOT NULL,
    -- The blob ID deterministically derived from the content of a blob and the Walrus configuration
    blob_id                     BYTEA         NOT NULL,
    -- ObjectID of the Blob object on Sui that owns the Metadata dynamic field
    owner_id                    BYTEA         NOT NULL,
    -- ID of the Metadata dynamic field
    dynamic_field_id            BYTEA         NOT NULL,
    -- Checkpoint sequence number that produced the state of the dynamic field
    cp_sequence_number          BIGINT        NOT NULL,
    -- Sentinel value to indicate whether the record is a tombstone
    deleted                     BOOLEAN       DEFAULT FALSE,
    PRIMARY KEY (address_owner, file_path)
);

-- This index supports querying on a path prefix for the blob_id. The partial index ignores records
-- marked for deletion.
CREATE INDEX IF NOT EXISTS walrus_blob_covering_idx ON walrus_blob
(address_owner, file_path TEXT_PATTERN_OPS)
INCLUDE (blob_id)
WHERE deleted = FALSE;

-- This index supports the pruner, which selects deletable records within a checkpoint range.
CREATE INDEX IF NOT EXISTS walrus_blob_can_delete_idx ON walrus_blob
(cp_sequence_number, deleted)
WHERE deleted = TRUE;

-- This table tracks historical changes to relevant Metadata dynamic fields. Unlike the main table,
-- the historical table is keyed on dynamic_field_id and df_version to capture the full history of
-- object changes.
CREATE TABLE IF NOT EXISTS walrus_blob_historical (
    -- ID of the Metadata dynamic field of key-value attributes on the Blob object
    dynamic_field_id            BYTEA         NOT NULL,
    -- Version of the Metadata dynamic field
    df_version                  BIGINT        NOT NULL,
    cp_sequence_number          BIGINT        NOT NULL,
    -- ObjectID of the Blob object on Sui that owns the Metadata dynamic field
    owner_id                    BYTEA,
    -- Address that owns the Blob object on Sui
    address_owner               BYTEA,
    file_path                   TEXT,
    -- The blob ID deterministically derived from the content of a blob and the Walrus configuration
    blob_id                     BYTEA,
    PRIMARY KEY (dynamic_field_id, df_version)
);

-- This index supports querying previous versions of a file on the exact file_path.
CREATE INDEX IF NOT EXISTS walrus_blob_historical_version_idx ON walrus_blob_historical
(address_owner, file_path, df_version DESC)
WHERE file_path IS NOT NULL;

-- Pruning on the historical table can be done by simply dropping records within the pruning range.
CREATE INDEX IF NOT EXISTS walrus_blob_historical_cp_sequence_number_idx ON walrus_blob_historical
(cp_sequence_number);

```

You could technically fulfill the requirements with just the historical table, but this significantly complicates read operations. For example, to get the latest files for an `address_owner`, your queries would need to:

1. Scan all historical records for the address.
2. Use complex window functions or self-joins to find the latest version per `file_path`.
3. Filter out deleted records manually.

This essentially rebuilds a subset of the `walrus_blob` table any time you invoke the main functionality of getting a user’s files. Because the primary goal is to track the current state of files (with occasional version history lookups), maintaining a separate table for the latest state dramatically simplifies and accelerates common read operations.

### Reads

With your tables built, you can construct the queries for your read functions.

Load paginated list of files:

```sql
SELECT blob_id
FROM walrus_blob 
WHERE address_owner = $1 AND deleted = FALSE
ORDER BY file_path
LIMIT $2;
```

Get file version history:

```sql
SELECT cp_sequence_number, dynamic_field_id, blob_id, owner_id
FROM walrus_blob_historical 
WHERE address_owner = $1 AND file_path = $2
ORDER BY cp_sequence_number DESC
LIMIT 20;
```

Filter file by path prefix:

```sql
SELECT blob_id
FROM walrus_blob 
WHERE address_owner = $1 
  AND file_path LIKE 'prefix%'
  AND deleted = FALSE
ORDER BY file_path
LIMIT $3;
```

## Indexing implementation

The indexer consists of two concurrent pipelines, a `WalrusBlobPipeline` that writes to the `walrus_blob` table, and a `WalrusBlobHistoricalPipeline` that writes to the `walrus_blob_historical` table.

| Pipeline | Granularity | Description |
| --- | --- | --- |
| `WalrusBlobPipeline` | Checkpoint-level | Any modifications made to the same dynamic field within a checkpoint are not reflected on the table, and only the final update is persisted. This is fine since the indexer assumes that `Metadata` dynamic fields can only be created or deleted. On commit, the `address_owner` and `file_path` columns are the primary keys to handle out-of-order writes. Filtering on the `cp_sequence_number` column ensures that upon constraint violation, the update persists only if it is newer than the existing row. |
| `WalrusBlobHistoricalPipeline` | Transaction-level | Writes all `Metadata` modifications to the `walrus_blob_historical` table. For some created, mutated, or unwrapped object, you can construct the relevant data directly from the output contents of its parent and itself. On the other hand, when an object is deleted or wrapped, it doesn't have an output version or contents, which complicates the indexing process. To avoid indexing more objects than necessary, check the object's input state, and if it is a relevant `Metadata` dynamic field, then write a tombstone record to the table. |

Crucially, the `walrus_blob` table is checkpoint-based because it tracks per-file path state. The `walrus_blob_historical` table tracks per-object history, and so it has a finer granularity.

Other options include implementing `WalrusBlobPipeline` as a sequential pipeline or writing to both tables with a single pipeline. Some experimentation is needed to identify the optimal pipeline setup for a table. For simple, sparse pipelines that don't typically do a lot of work per checkpoint, either sequential or concurrent pipelines might be sufficient. For upsert tables, the choice often comes down to balancing out-of-order processing complexity and contention against the reduced throughput from sequential processing.

### Object changes of interest

When creating, mutating, wrapping or deleting, or unwrapping the `Metadata` dynamic field, it appears among the output objects in the object changes of a transaction, such as this [transaction that creates the field](https://suivision.xyz/txblock/3Qcuo2FaTZL5wfdi7JzPELcmkuZm7hVfdNrkLrdkKioN?tab=Changes). Because the service limits users to creating and deleting objects only, index dynamic fields of type `0x2::dynamic_field::Field<vector<u8>, 0xfdc88f7d7cf30afab2f82e8380d11ee8f70efb90e863d1de8616fae1bb09ea77::metadata::Metadata>` among the transactions’ object changes per the following table. You can get the `address_owner` and `blob_id` values from the parent Sui `Blob` object, which must also appear in the transaction if the child `Metadata` dynamic field is present.

| Object change to `Metadata` dynamic field | In input objects | In live output objects | `walrus_blob_historical` table | `walrus_blob` table |
| --- | --- | --- | --- | --- |
| Creation | ❌ | ✅ | New entry | Upsert entry  |
| Deletion | ✅ | ❌ | New tombstone row (optional fields left NULL) | There must be an existing entry, and the entry’s `deleted` column is set to `TRUE` |

## WalrusBlobPipeline

The functions that define the `WalrusBlobPipeline`.

### Process

The `process` function receives checkpoints from the ingestion task and transforms the data into `ProcessedWalrusMetadata`. For this pipeline, first compute the `checkpoint_input_objects` and `latest_live_output_objects`, as you need to know only the state of objects entering and exiting the checkpoint for the purpose of the `walrus_blob` table.

`Metadata` dynamic fields that exist in `checkpoint_input_objects` but not `latest_live_output_objects` must have been wrapped or deleted. The `process` function is responsible for finding the input state of the dynamic field’s parent so that the correct record could be referenced from its contents and tombstoned.

For relevant data in `latest_live_output_objects`, the `process` function instead extracts the output state of the dynamic field’s parent. Because mutating dynamic fields is restricted, ignore any that appear in both `checkpoint_input_objects` and `latest_live_output_objects`.

Link + snippet of process function

### Commit

The `commit` function receives the transformed data from `process` and conducts final transformations, if any, before writing out to the store. Handle out-of-order writes by using the `address_owner` and `file_path` columns as the primary key, and filtering on the `cp_sequence_number` column to ensure that on constraint violation, the update persists only if it is newer than the existing row.

Link + snippet of commit function

### Prune

When a retention policy is set, the indexer framework ensures that for the latest committed checkpoint, represented by `checkpoint_hi_inclusive` on the watermarks table, the `reader_lo` representing the minimum available checkpoint is the smallest checkpoint allowed per the retention. The `pruner_hi` serves as the starting checkpoint to prune from.

While the pruner task runs, checkpoints between `[pruner_hi, reader_lo)` are periodically pruned. Because entries on both `walrus_blob` and `walrus_blob_historical` are stamped with `cp_sequence_number`, you can delete records on `walrus_blob` where `cp_sequence_number` is within `[pruner_hi, reader_lo)` and `deleted=TRUE`. On `walrus_blob_historical`, you can delete all records older or within the pruning range, as the latest version of the file is available on the main table.

Link + snippet of prune function

## WalrusBlobHistoricalPipeline

The functions that define the `WalrusBlobHistoricalPipeline`.

### Process

Unlike the other table, `walrus_blob_historical` tracks all object changes. Consequently, the processing is done on a transaction-level. Otherwise, the checks are largely the same, except in the case of a field being wrapped or deleted. Because the historical table is keyed on dynamic field ID and version, you need to check only that the input state of the dynamic field has a path-attribute key-value pair. Its parent object contents don't need to be extracted, because those fields are to remain empty for tombstone rows.

### Commit

Because `walrus_blob_historical` is an append-only table, you can just write all data out to the table and ignore any conflicts.

### Prune

Because `walrus_blob` is solely responsible for returning the live object set, you can safely prune obsolete entries on `walrus_blob_historical`.

## Additional considerations

Other details to keep in mind when indexing Walrus blobs. 

### Lifetimes and expiration

All Walrus blobs have a lifetime associated with them, so you also need to track changes to expiration. Whenever the `Metadata` dynamic field is changed, the parent Sui `Blob` object should also appear in output changes. You can determine the lifetime of the blob from the `Blob` object’s contents. However, lifetime changes to the blob typically occur on the `Blob` object itself. Because updates to the parent object don't affect the child dynamic field - unless the child dynamic field is directly modified - these lifetime changes remain invisible to the current indexing setup. There are a few ways to address this:

- Watch all `Blob` object changes.
- Watch all `BlobCertified` events.
- Constructs PTBs that make calls to manage blob lifetime **and** ping the `Metadata` dynamic field in the same transaction.

If a builder finds it undesirable to do additional work on the write side, then they are limited to the first two options. This necessitates two pipelines, one to do the work in the previous section of indexing metadata, and another to index `BlobCertified` events (or `Blob` object changes.) 

Otherwise, you can continue using a single pipeline. 

### Mutations to a dynamic field

In this design, if a user wants to `move` a file through the service, they would have to upload contents to a new file path, and then delete the existing file. On Sui, this would create a new `Blob` object, a new `Metadata` dynamic field on the `Blob` object, and then delete the existing `Blob` object and `Metadata`. This is a bit convoluted, as there is a method on the smart contract to [directly update the existing Metadata](https://github.com/MystenLabs/walrus/blob/main/contracts/walrus/sources/system/blob.move#L355). However, these mutations introduce additional complexity for the indexer.

Because the `walrus_blob` table is keyed on `(address_owner, file_path)`, if you rename some metadata such that `(df_id, "test/a")` → `(df_id, "test/b")`, the indexer records the change as a new entry. This culminates in the read query erroneously reporting that the user has two files, `test/a` and `test/b`. On the other hand, if you update `walrus_blob` to key on dynamic field ID instead, overwrites (new files uploaded to the same file path) are treated as entirely separate files instead of later versions of the same file. In essence, a table keyed on `file_path` can handle overwrites, but not renames, while a table keyed on `dynamic_field_id` can track renames, but not overwrites. 

One possible approach is to detect rename mutations (input state of dynamic field is different from output state of dynamic field) and commit two writes to `walrus_blob`. One write to set the existing row as a sentinel row, and then another write to create a new entry to reflect the rename. The historical table also needs a tombstone for the old path and a new entry for the new path. 

## Related links

- [Walrus Docs](https://docs.wal.app/): Walrus is a decentralized storage and data availability protocol designed specifically for large binary files, or blobs.
- [Custom Indexer](../custom-indexer.mdx): Build custom indexers using the Sui micro-data ingestion framework.
- [Dynamic Fields](../../../../concepts/dynamic-fields.mdx): Dynamic fields and dynamic object fields on Sui are added and removed dynamically, affect gas only when accessed, and store heterogeneous values.