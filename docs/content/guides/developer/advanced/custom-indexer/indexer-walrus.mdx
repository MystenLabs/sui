---
title: Using the Custom Indexer Framework to Index Walrus Metadata Attributes
description: Walrus is a content-addressable storage protocol, where data is retrieved using a unique identifier derived from the content itself, rather than from a file path or location. Integrating a custom Sui Indexer with Walrus can provide novel user experiences.
---

This topic covers integration of the Sui Indexer with the Walrus protocol. Walrus is a decentralized storage and data availability protocol designed specifically for large binary files, or blobs. Walrus is operated and controlled by the Walrus Foundation. For the most accurate and up-to-date inormation on Walrus, consult the official [Walrus Docs](https://docs.wal.app/).

Walrus is a content-addressable storage protocol, where data is retrieved using a unique identifier derived from the content itself, rather than from a file path or location. Each blob file stored on Walrus is represented by a corresponding [`Blob` object on Sui](https://docs.wal.app/dev-guide/sui-struct.html#blob-and-storage-objects), and this associated object can optionally have a `Metadata` [dynamic field](../../../../concepts/dynamic-fields.mdx) (key-value extensions to on-chain objects to flexibly extend an object's data at runtime). If set, this dynamic field is [a mapping of key-value attribute pairs](https://docs.wal.app/usage/client-cli.html?highlight=attribute#blob-attributes).

This guide shows how to index and query this metadata through a simple blog platform example. Users can:
- Upload blog posts with titles
- View their own posts and metrics
- Delete posts they created
- Edit post titles
- Browse posts by other publishers

Assume an existing service that handles uploads to Walrus. When a Blob and its associated object is created on Walrus, the service will additionally create a Metadata dynamic field with key-value pairs for `publisher` - the Sui Address that uploaded the blob, `blob_id`, `view_count`, and `title`. The service restricts users from modifying the `publisher`, `blob_id`, and `view_count` pairs, but the `title` can be updated by the respective publisher.

Then, to support the above features, you can use the Custom Indexer Framework to track these Metadata dynamic fields.

## Data modeling

One way to model this is with a single table that maps publisher addresses to their blog posts. The table is keyed on `dynamic_field_id`, which uniquely identifies each Metadata dynamic field on Sui. For context, when a publisher uploads a blog post, Walrus will either create a new Blob, or reuse an existing one if the content already exists on-chain. As such, the Blob ID is not a reliable primary key. Fortunately, each upload also creates a unique Sui Blob object. When you add the Metadata attribute dynamic field to this Sui Blob object, the dynamic field's ID is derived from the parent object, and so you can guarantee primary key uniqueness, while using the ID that is closest to where your dapp data lies.

{@inject: examples/rust/walrus-attributes-indexer/migrations/2025-08-11-194316_blog-post/up.sql}

You could technically fulfill the requirements with just the historical table, but this significantly complicates read operations. For example, to get the latest files for an `address_owner`, your queries would need to:

1. Scan all historical records for the address.
2. Use complex window functions or self-joins to find the latest version per `file_path`.
3. Filter out deleted records manually.

This essentially rebuilds a subset of the `walrus_blob` table any time you invoke the main functionality of getting a user’s files. Because the primary goal is to track the current state of files (with occasional version history lookups), maintaining a separate table for the latest state dramatically simplifies and accelerates common read operations.

### Reads

The main query pattern is to load the blog posts of a publisher.


```sql
SELECT *
FROM blog_post
WHERE publisher = $1
ORDER BY title
LIMIT $2;
```

## Indexing implementation

The indexer consists of two concurrent pipelines, a `WalrusBlobPipeline` that writes to the `walrus_blob` table, and a `WalrusBlobHistoricalPipeline` that writes to the `walrus_blob_historical` table.

| Pipeline | Granularity | Description |
| --- | --- | --- |
| `WalrusBlobPipeline` | Checkpoint-level | Any modifications made to the same dynamic field within a checkpoint are not reflected on the table, and only the final update is persisted. This is fine since the indexer assumes that `Metadata` dynamic fields can only be created or deleted. On commit, the `address_owner` and `file_path` columns are the primary keys to handle out-of-order writes. Filtering on the `cp_sequence_number` column ensures that upon constraint violation, the update persists only if it is newer than the existing row. |
| `WalrusBlobHistoricalPipeline` | Transaction-level | Writes all `Metadata` modifications to the `walrus_blob_historical` table. For some created, mutated, or unwrapped object, you can construct the relevant data directly from the output contents of its parent and itself. On the other hand, when an object is deleted or wrapped, it doesn't have an output version or contents, which complicates the indexing process. To avoid indexing more objects than necessary, check the object's input state, and if it is a relevant `Metadata` dynamic field, then write a tombstone record to the table. |

Crucially, the `walrus_blob` table is checkpoint-based because it tracks per-file path state. The `walrus_blob_historical` table tracks per-object history, and so it has a finer granularity.

Other options include implementing `WalrusBlobPipeline` as a sequential pipeline or writing to both tables with a single pipeline. Some experimentation is needed to identify the optimal pipeline setup for a table. For simple, sparse pipelines that don't typically do a lot of work per checkpoint, either sequential or concurrent pipelines might be sufficient. For upsert tables, the choice often comes down to balancing out-of-order processing complexity and contention against the reduced throughput from sequential processing.

### Object changes of interest

When creating, mutating, wrapping or deleting, or unwrapping the `Metadata` dynamic field, it appears among the output objects in the object changes of a transaction, such as this [transaction that creates the field](https://suivision.xyz/txblock/3Qcuo2FaTZL5wfdi7JzPELcmkuZm7hVfdNrkLrdkKioN?tab=Changes). Because the service limits users to creating and deleting objects only, index dynamic fields of type `0x2::dynamic_field::Field<vector<u8>, 0xfdc88f7d7cf30afab2f82e8380d11ee8f70efb90e863d1de8616fae1bb09ea77::metadata::Metadata>` among the transactions’ object changes per the following table. You can get the `address_owner` and `blob_id` values from the parent Sui `Blob` object, which must also appear in the transaction if the child `Metadata` dynamic field is present.

| Object change to `Metadata` dynamic field | In input objects | In live output objects | `walrus_blob_historical` table | `walrus_blob` table |
| --- | --- | --- | --- | --- |
| Creation | ❌ | ✅ | New entry | Upsert entry  |
| Deletion | ✅ | ❌ | New tombstone row (optional fields left NULL) | There must be an existing entry, and the entry’s `deleted` column is set to `TRUE` |

## WalrusBlobPipeline

The functions that define the `WalrusBlobPipeline`.

### Process

The `process` function receives checkpoints from the ingestion task and transforms the data into `ProcessedWalrusMetadata`. For this pipeline, first compute the `checkpoint_input_objects` and `latest_live_output_objects`, as you need to know only the state of objects entering and exiting the checkpoint for the purpose of the `walrus_blob` table.

`Metadata` dynamic fields that exist in `checkpoint_input_objects` but not `latest_live_output_objects` must have been wrapped or deleted. The `process` function is responsible for finding the input state of the dynamic field’s parent so that the correct record could be referenced from its contents and tombstoned.

For relevant data in `latest_live_output_objects`, the `process` function instead extracts the output state of the dynamic field’s parent. Because mutating dynamic fields is restricted, ignore any that appear in both `checkpoint_input_objects` and `latest_live_output_objects`.

Link + snippet of process function

### Commit

The `commit` function receives the transformed data from `process` and conducts final transformations, if any, before writing out to the store. Handle out-of-order writes by using the `address_owner` and `file_path` columns as the primary key, and filtering on the `cp_sequence_number` column to ensure that on constraint violation, the update persists only if it is newer than the existing row.

Link + snippet of commit function

### Prune

When a retention policy is set, the indexer framework ensures that for the latest committed checkpoint, represented by `checkpoint_hi_inclusive` on the watermarks table, the `reader_lo` representing the minimum available checkpoint is the smallest checkpoint allowed per the retention. The `pruner_hi` serves as the starting checkpoint to prune from.

While the pruner task runs, checkpoints between `[pruner_hi, reader_lo)` are periodically pruned. Because entries on both `walrus_blob` and `walrus_blob_historical` are stamped with `cp_sequence_number`, you can delete records on `walrus_blob` where `cp_sequence_number` is within `[pruner_hi, reader_lo)` and `deleted=TRUE`. On `walrus_blob_historical`, you can delete all records older or within the pruning range, as the latest version of the file is available on the main table.

Link + snippet of prune function

## WalrusBlobHistoricalPipeline

The functions that define the `WalrusBlobHistoricalPipeline`.

### Process

Unlike the other table, `walrus_blob_historical` tracks all object changes. Consequently, the processing is done on a transaction-level. Otherwise, the checks are largely the same, except in the case of a field being wrapped or deleted. Because the historical table is keyed on dynamic field ID and version, you need to check only that the input state of the dynamic field has a path-attribute key-value pair. Its parent object contents don't need to be extracted, because those fields are to remain empty for tombstone rows.

### Commit

Because `walrus_blob_historical` is an append-only table, you can just write all data out to the table and ignore any conflicts.

### Prune

Because `walrus_blob` is solely responsible for returning the live object set, you can safely prune obsolete entries on `walrus_blob_historical`.

## Additional considerations

Other details to keep in mind when indexing Walrus blobs.

### Lifetimes and expiration

All Walrus blobs have a lifetime associated with them, so you also need to track changes to expiration. Whenever the `Metadata` dynamic field is changed, the parent Sui `Blob` object should also appear in output changes. You can determine the lifetime of the blob from the `Blob` object’s contents. However, lifetime changes to the blob typically occur on the `Blob` object itself. Because updates to the parent object don't affect the child dynamic field - unless the child dynamic field is directly modified - these lifetime changes remain invisible to the current indexing setup. There are a few ways to address this:

- Watch all `Blob` object changes.
- Watch all `BlobCertified` events.
- Constructs PTBs that make calls to manage blob lifetime **and** ping the `Metadata` dynamic field in the same transaction.

If a builder finds it undesirable to do additional work on the write side, then they are limited to the first two options. This necessitates two pipelines, one to do the work in the previous section of indexing metadata, and another to index `BlobCertified` events (or `Blob` object changes.)

Otherwise, you can continue using a single pipeline.

### Mutations to a dynamic field

In this design, if a user wants to `move` a file through the service, they would have to upload contents to a new file path, and then delete the existing file. On Sui, this would create a new `Blob` object, a new `Metadata` dynamic field on the `Blob` object, and then delete the existing `Blob` object and `Metadata`. This is a bit convoluted, as there is a method on the smart contract to [directly update the existing Metadata](https://github.com/MystenLabs/walrus/blob/main/contracts/walrus/sources/system/blob.move#L355). However, these mutations introduce additional complexity for the indexer.

Because the `walrus_blob` table is keyed on `(address_owner, file_path)`, if you rename some metadata such that `(df_id, "test/a")` → `(df_id, "test/b")`, the indexer records the change as a new entry. This culminates in the read query erroneously reporting that the user has two files, `test/a` and `test/b`. On the other hand, if you update `walrus_blob` to key on dynamic field ID instead, overwrites (new files uploaded to the same file path) are treated as entirely separate files instead of later versions of the same file. In essence, a table keyed on `file_path` can handle overwrites, but not renames, while a table keyed on `dynamic_field_id` can track renames, but not overwrites.

One possible approach is to detect rename mutations (input state of dynamic field is different from output state of dynamic field) and commit two writes to `walrus_blob`. One write to set the existing row as a sentinel row, and then another write to create a new entry to reflect the rename. The historical table also needs a tombstone for the old path and a new entry for the new path.

## Related links

- [Walrus Docs](https://docs.wal.app/): Walrus is a decentralized storage and data availability protocol designed specifically for large binary files, or blobs.
- [Custom Indexer](../custom-indexer.mdx): Build custom indexers using the Sui micro-data ingestion framework.
- [Dynamic Fields](../../../../concepts/dynamic-fields.mdx): Dynamic fields and dynamic object fields on Sui are added and removed dynamically, affect gas only when accessed, and store heterogeneous values.
